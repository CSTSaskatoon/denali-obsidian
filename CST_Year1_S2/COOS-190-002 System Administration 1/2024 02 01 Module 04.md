# Storage Spaces
### Manage Disk failure with Storage Spaces
To enhance disk-fault tolerance
- Design a complete fault tolerant storage solution
- Deploy a highly available storage pool
- Verify hardware and firmware components
- Replace failed disks immediately
- Retain some unallocated space
- Be prepared for multiple disk failures
- Provide fault tolerance at the enclosure level
# Implement Data deduplication
### What is data deduplication
The data deduplication functionality in windows server optimizes free storage on volumes by searching for duplicated portions of files and storing them only once
- Reduces the impact of redundant data on storage size and cost
- Optimizes free space
- Optionally compresses data for additional savings
- Space savings depend on dataset or workload on the volume
- Optimization rats up to 90% or 20x reduction in storage utilization
- Chunks are where it stores the duplicated data
- Reparse point is what the file used to be, a bunch of pointers going to chunks
Transparently removes duplication without changing access schematics
When you enable Data Deduplication on a volume, a post-process (target) deduplication is used to optimize the file data on the volume by performing the following actions
- Process the files on the volume by using optimized jobs, which are background task un with low priority on the server
- Uses an algorithm to segment all file data on the volume into small, variable sized chunks that range from 32 to 128 KB
- Identifies chunks that have one or more duplicates on a volume
- Inserts chunks into a common chunk store
- Replaces all duplicate chunks with a reference (or stub) to a single data-chunk copy in the chunk store
- Replaces the original files with a reparse point which contains references to it's data chunks
- Compresses chunks and organizes them in container which contains references to it's data chunks
- Compresses chunks and organizes them in container files in the System Volume Information folder
- Removes primary data stream of files
After a volume is data deduplicated and optimized it contains
- Unoptimized files that are not in policy (files skipped over by deduplication)
- Optimized files stored as reparse points
- Chunk store (the optimized file data, stored as chunks packed into container files)
When new files are added, they are not optimized right away
The chunk store consists of one or more chunk store container files. New chunks are appended to the current chunk storage container
When an optimized file is deleted, it's reparse point is deleted but it's data chunks are not immediately deleted from the chunk store
Unoptimized files include
- Files that don't meet the selected file-age policy
- System sate files
- Alternate Data streams
- Encrypted files
- Files with extended attributes
- Files smaller than 32KB
- Other reparse point files
Deduplication is not supported on
- System or boot volumes
- Remote mapped or remote mounted drives
- Cluster shared volume file (CSVFS) for non-VDI workloads or any workloads on Windows Server 2012
- Files approaching or larger than 1 TB in size
- Volumes approaching or larger than 64TB in size
The data Deduplication role service consists of several components including
- Filter driver which monitors local or remote input/output (I/O) - one filter per volume
- Optimization
- Garbage collection
- Integrity Scrubbing
- Un-optimization
### Data Deduplication Components
*Integrity Scrubbing* - Performs data-integrity verification, such as checksum validation and metadata consistency
As data is accessed or deduplication jobs process data, if these features encounter corruption they record it in a log file
Scrubbing jobs use these features to analyze the chunk-store corruption logs and make repairs when possible
Possible repair Operations
- Backup Copies
- Mirror image
- New Chunk
Diagram
	![600](Pasted%20image%2020240201140249.png)
	![600](Pasted%20image%2020240201140614.png)
There's one deduplication process but a different filter on each drive
### Deploy Data Deduplication
Determine your target documents
Deciding which volumes are deduplication candidates
Evaluating potential savings with the Deduplication Evaluation Tool (DDPEval.exe)
When it's enabled it creates default encryption policy settings
You need to process files more quickly and you know that incoming data is static or read-only
### Usage Scenarios
User documents - includes group content publication or sharing (30-50%)
Software Deployment Shares - (70-80%)
Virtualization Libraries - (80-95%)
General file share - (50-60%)
You can use Data Deduplication as
- General purpose file servers
- Virtual Desktop Infrastructure (VDI) deployments
- VM deployments
- Backup servers
### Monitor and Maintain Data Deduplication
Candidate that aren't ideal
- Hyper-V hosts
- Windows Server Update Service (WSUS)
- SQL Server and Exchange Server database volumes
Monitor Data Deduplication by using
- Windows PowerShell cmdlets
- Event viewer logs
- Performance Monitor
- File Explorer
Maintain data deduplication by using PowerShell cmdlets
### Backup and Restore considerations with Data Deduplication
Smaller backup and restore sizes
Faster full-volume restores